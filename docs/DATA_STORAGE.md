# Data Storage Architecture

## Overview

OrgMind uses a **multi-layer storage system** with in-memory processing, file-based persistence, and runtime caching.

---

## Storage Layers

### 1. **Source Data Layer** (Raw)
ğŸ“ **Location**: `/backend/data/raw/company_emails.csv`

**Format**: CSV (Comma-Separated Values)

**Structure**:
```csv
id,date,sender,to,cc,subject,body
1,2024-01-15T09:30:00Z,john.smith@company.com,engineering-team@company.com,...
```

**Purpose**: 
- Human-readable source data
- Easy to edit and add new emails
- 20 emails currently stored

**Pros**:
- âœ… Easy to edit manually
- âœ… Version control friendly
- âœ… Can be viewed in spreadsheets

**Cons**:
- âŒ Not optimized for querying
- âŒ No relationships stored
- âŒ Linear structure only

---

### 2. **Knowledge Graph Layer** (Processed)
ğŸ“ **Location**: `/backend/data/processed/knowledge_graph.pkl`

**Format**: Python Pickle (Binary)

**Size**: 4.7 KB

**Structure**: Serialized `GraphBuilder` object containing:
- NetworkX graph (nodes + edges)
- Node attributes (type, label, metadata)
- Edge attributes (relation_type, weight)

**How It's Created**:
```python
# From load_real_data.py
graph = GraphBuilder()
graph.add_entity("John Smith", "John Smith", {"type": "person"})
graph.add_decision("decision_1", content="...", date="...")
graph.add_relation("decision_1", "John Smith", "made_by")
graph.save("data/processed/knowledge_graph.pkl")
```

**Internal Structure**:
```python
GraphBuilder
â”œâ”€â”€ _graph: networkx.DiGraph
â”‚   â”œâ”€â”€ nodes: dict[node_id, {label, type, ...}]
â”‚   â””â”€â”€ edges: dict[(source, target), {relation_type, weight}]
â””â”€â”€ Methods: add_entity(), add_relation(), save(), load()
```

**Purpose**:
- Fast graph queries
- Relationship traversal
- Entity connections
- Quick load on startup

**Pros**:
- âœ… Fast to load (4.7KB â†’ milliseconds)
- âœ… Preserves graph structure
- âœ… Supports complex queries
- âœ… NetworkX powers graph algorithms

**Cons**:
- âŒ Binary format (not human-readable)
- âŒ Python-specific (not portable)
- âŒ No concurrent writes
- âŒ Loses data if corrupted

---

### 3. **Runtime Memory Layer** (Active)
ğŸ“¦ **Location**: Backend process memory

**Format**: Python objects in RAM

**Structure**:
```python
# main.py - Shared state
graph_builder = GraphBuilder()        # In-memory graph
coordinator = Coordinator(graph=...)  # Agent system
app.state.graph_cache = {...}         # FastAPI cache
```

**What's Stored**:
1. **graph_builder**: The live NetworkX graph
2. **coordinator**: Agent memory and reasoning history
3. **app.state.graph_cache**: Exported JSON for API responses
4. **app.state.graph_history**: Version history timeline
5. **app.state.stats**: Request statistics

**Purpose**:
- Ultra-fast queries (no disk I/O)
- Real-time modifications
- Agent reasoning state
- API response caching

**Lifecycle**:
```
Backend Start
  â†“
Load knowledge_graph.pkl â†’ graph_builder (RAM)
  â†“
Serve API requests (use RAM)
  â†“
Optional: Save modified graph back to .pkl
  â†“
Backend Stop (RAM cleared)
```

**Pros**:
- âœ… Instant access (microseconds)
- âœ… Supports rapid modifications
- âœ… No file locks

**Cons**:
- âŒ Lost on crash/restart
- âŒ Limited by RAM size
- âŒ No persistence without explicit save

---

### 4. **API Response Cache** (Temporary)
ğŸ“¦ **Location**: `app.state.graph_cache` (FastAPI application state)

**Format**: Python dictionary (JSON-serializable)

**Structure**:
```python
{
    "nodes": [
        {"id": "John Smith", "label": "John Smith", "type": "person"},
        ...
    ],
    "edges": [
        {"source": "decision_1", "target": "John Smith", "relation_type": "made_by"},
        ...
    ],
    "metadata": {
        "version": 0,
        "node_count": 72,
        "edge_count": 7,
        "loaded_from_disk": true
    }
}
```

**Purpose**:
- Avoid re-exporting graph on every API call
- Fast JSON responses
- Version tracking

**When Updated**:
- On backend startup
- After processing new information
- After demo scenarios run
- On manual refresh

---

## Data Flow

### Initial Load (Backend Startup)

```
1. Backend starts (main.py)
   â†“
2. Check if knowledge_graph.pkl exists
   â†“
3a. IF EXISTS:
    â†’ Load pickle â†’ Deserialize GraphBuilder â†’ Set graph_builder
   â†“
3b. IF NOT EXISTS:
    â†’ Generate mock data â†’ Build graph in memory
   â†“
4. Export to app.state.graph_cache (JSON format)
   â†“
5. Ready to serve API requests
```

### Adding New Data

```
1. CSV: Add email to company_emails.csv
   â†“
2. Run: python scripts/load_real_data.py
   â†“
3. Script:
   - Reads CSV
   - Builds GraphBuilder
   - Saves to knowledge_graph.pkl
   â†“
4. Restart backend
   â†“
5. Backend loads new .pkl file
   â†“
6. New data appears in UI
```

### Runtime Processing

```
User Query via API
   â†“
POST /process or POST /query
   â†“
Coordinator.process(info) [in memory]
   â†“
- MemoryAgent updates graph
- RouterAgent finds connections  
- CriticAgent validates
   â†“
graph_builder modified (RAM only)
   â†“
app.state.graph_cache updated
   â†“
Return JSON response
   â†“
(Changes in RAM but not persisted to .pkl)
```

---

## Storage Technology Stack

### Core Libraries

1. **NetworkX** (Graph Storage)
   - Python library for complex networks
   - Directed graph (DiGraph)
   - Node/edge attributes
   - Graph algorithms (shortest path, centrality, etc.)

2. **Pickle** (Serialization)
   - Python's native serialization
   - Binary format
   - Fast load/save
   - Preserves Python objects exactly

3. **Pandas** (CSV Processing)
   - Reads company_emails.csv
   - Data manipulation
   - Easy filtering/transformation

4. **FastAPI State** (Runtime Cache)
   - Application-level state management
   - Shared across all requests
   - Persists during server lifetime

---

## Data Persistence Strategy

### What Gets Saved

âœ… **Persisted to Disk**:
- Source emails (CSV)
- Compiled knowledge graph (.pkl)

âŒ **Not Persisted** (RAM only):
- Agent reasoning history
- Query results
- Session state
- Runtime modifications

### When Data is Saved

**Automatic Save**: NEVER (currently)
- Changes in RAM are not auto-saved to disk

**Manual Save Options**:
1. Run `load_real_data.py` script (rebuilds from CSV)
2. Add save logic to API endpoints
3. Periodic background saves (not implemented)

### Data Loss Scenarios

âš ï¸ **You Will Lose**:
- Runtime graph modifications (unless saved)
- Agent reasoning history
- Query results
- Demo scenario changes

ğŸ”’ **You Won't Lose**:
- Original CSV data
- Last saved .pkl graph
- Configuration files

---

## File Structure

```
backend/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ company_emails.csv       # Source data (editable)
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ knowledge_graph.pkl       # Compiled graph (binary)
â”œâ”€â”€ main.py                           # Backend server (loads .pkl)
â”œâ”€â”€ knowledge_graph.py                # GraphBuilder class
â””â”€â”€ scripts/
    â””â”€â”€ load_real_data.py             # CSV â†’ PKL converter
```

---

## Storage Comparison

| Layer | Format | Size | Speed | Persistent | Editable |
|-------|--------|------|-------|------------|----------|
| CSV | Text | ~10KB | Slow | âœ… Yes | âœ… Yes |
| Pickle | Binary | 4.7KB | Fast | âœ… Yes | âŒ No |
| RAM | Objects | ~1MB | Instant | âŒ No | âœ… Yes |
| Cache | JSON | ~50KB | Very Fast | âŒ No | âŒ No |

---

## Future Improvements

### Recommended Enhancements

1. **Add Database (PostgreSQL + pgvector)**
   ```python
   # Store vectors for semantic search
   # Support concurrent writes
   # Transaction safety
   ```

2. **Auto-Save on Modifications**
   ```python
   @app.post("/process")
   async def process_info(info: NewInformation):
       result = coordinator.process(info)
       graph_builder.save("data/processed/knowledge_graph.pkl")
       return result
   ```

3. **Version Control**
   ```python
   # Save timestamped versions
   graph_builder.save(f"data/processed/graph_{timestamp}.pkl")
   ```

4. **Backup Strategy**
   ```bash
   # Daily backups
   cp knowledge_graph.pkl backups/graph_$(date +%Y%m%d).pkl
   ```

5. **Export Formats**
   ```python
   # GraphML (Neo4j compatible)
   # JSON (portable)
   # GraphSON (Gremlin)
   ```

---

## How to Inspect Data

### View CSV Data
```bash
cat backend/data/raw/company_emails.csv
```

### Check Pickle Contents
```python
import pickle
with open('backend/data/processed/knowledge_graph.pkl', 'rb') as f:
    graph = pickle.load(f)
    print(graph.get_stats())
```

### Query via API
```bash
curl http://localhost:8000/graph | python -m json.tool
```

### Direct NetworkX Access
```python
from knowledge_graph import GraphBuilder
graph = GraphBuilder.load('data/processed/knowledge_graph.pkl')
nx_graph = graph.get_graph()
print(nx_graph.nodes(data=True))
print(nx_graph.edges(data=True))
```

---

## Summary

**Storage Flow**:
```
CSV (source)
  â†“ [load_real_data.py]
Pickle (persistent graph)
  â†“ [backend startup]
RAM (active graph)
  â†“ [API export]
JSON (frontend display)
```

**Key Points**:
- ğŸ“ Edit CSV for new data
- ğŸ”„ Run script to rebuild graph
- ğŸ’¾ Pickle stores compiled graph
- âš¡ RAM powers fast queries
- ğŸŒ JSON serves frontend
- âš ï¸ Runtime changes not auto-saved

---

**Last Updated**: February 2026
